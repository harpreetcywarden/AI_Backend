import os
import csv
import base64
import uuid 
import time
from datetime import datetime, timezone
import pytesseract
import pandas as pd
import tempfile
import shutil
import logging
import boto3
from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ClientError
from pydantic import BaseModel, Field
from typing import List, Optional
from dotenv import load_dotenv
from datetime import datetime # Optional for timestamping

from fastapi import FastAPI, Depends, HTTPException, status, BackgroundTasks, Request, Query 
from pydantic import BaseModel, Field, model_validator

from langchain_experimental import SemanticChunker
from langchain_community.document_loaders import PyPDFLoader
from sentence_transformers import SentenceTransformer
from opensearchpy import OpenSearch, RequestsHttpConnection, exceptions as opensearch_exceptions
from opensearchpy.exceptions import NotFoundError
from docx import Document as DocxDocument
from pptx import Presentation
from PIL import Image
from pdf2image import convert_from_path

# --- Load Environment Variables ---
load_dotenv()
print("Attempting to load environment variables...")

# --- Configuration from Environment Variables ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# API Key
VERIFY_API_KEY_VALUE = os.getenv("MY_PROCESSING_API_KEY")
if not VERIFY_API_KEY_VALUE:
    logger.warning("‚ö†Ô∏è MY_PROCESSING_API_KEY not found in environment variables. API key check will fail.")
    
    
class SearchRequestPayload(BaseModel):
    user_id: str
    query: str
    top_k: int = Field(default=5, gt=0, le=50) # Default to 5 results, min 1, max 50 (adjustable)
    # Optional: Add a minimum score threshold if needed
    # min_score: Optional[float] = Field(default=None, ge=0.0, le=1.0)

class HistoryEntryResponse(BaseModel): # For retrieving history
    user_id: str
    session_id: Optional[str] = None
    human_message: Optional[str] = None
    ai_message: Optional[str] = None
    timestamp: datetime

class HistoryEntryResponse(BaseModel): # For retrieving history
    user_id: str
    session_id: Optional[str] = None
    human_message: Optional[str] = None
    ai_message: Optional[str] = None
    timestamp: datetime

class SearchResult(BaseModel):
    chunk_id: int
    content: str
    original_filename: str
    s3_key: str  # Include s3_key for context
    entry_uuid: str # Include entry_uuid for context
    score: float # Similarity score from OpenSearch

class SearchResponse(BaseModel):
    results: List[SearchResult]
    index_queried: str # Inform which index was used

class ProcessRequestPayload(BaseModel):
    user_id: str
    user_email: Optional[str] = None
    original_filename: str
    entry_uuid: str
    s3_key: str
    status: str

class ChatHistoryPayload(BaseModel): # For storing history
    user_id: str = Field(..., description="The ID of the user initiating the chat turn.")
    session_id: Optional[str] = Field(None, description="An identifier for the chat session.")
    human_message: Optional[str] = Field(None, description="The message sent by the human user.")
    ai_message: Optional[str] = Field(None, description="The message generated by the AI.")

    # Ensure at least one message is present
    @model_validator(mode='after')
    def check_messages_present(cls, values):
        if not values.human_message and not values.ai_message:
            raise ValueError('At least one of human_message or ai_message must be provided')
        return values


# S3 Configuration
S3_BUCKET = os.getenv("AWS_STORAGE_BUCKET_NAME")
S3_REGION = os.getenv("AWS_S3_REGION_NAME")
s3_client = None # Initialize as None
if not S3_BUCKET or not S3_REGION:
    logger.error("‚ùå AWS_STORAGE_BUCKET_NAME or AWS_S3_REGION_NAME not set in environment variables.")
else:
    try:
        logger.info(f"Attempting to initialize S3 client for bucket '{S3_BUCKET}' in region '{S3_REGION}'...")
        s3_client = boto3.client('s3', region_name=S3_REGION)
        s3_client.head_bucket(Bucket=S3_BUCKET)
        logger.info(f"‚úÖ S3 client initialized and bucket '{S3_BUCKET}' accessible.")
    except (NoCredentialsError, PartialCredentialsError) as cred_err:
        logger.error(f"‚ùå AWS credentials error: {cred_err}")
        s3_client = None # Ensure client is None on error
    except ClientError as e:
        error_code = e.response.get("Error", {}).get("Code")
        logger.error(f"‚ùå S3 ClientError ({error_code}): {e}", exc_info=(error_code not in ['NoSuchBucket', 'AccessDenied']))
        s3_client = None # Ensure client is None on error
    except Exception as e:
        logger.error(f"‚ùå An unexpected error occurred during S3 client initialization: {e}", exc_info=True)
        s3_client = None # Ensure client is None on error


# Tesseract Configuration
try:
    pytesseract.pytesseract.get_tesseract_version()
    logger.info(f"‚úÖ Tesseract found at: {pytesseract.pytesseract.tesseract_cmd}")
except pytesseract.TesseractNotFoundError:
    logger.error("‚ùå Tesseract is not installed or not found in PATH. OCR functionality will fail.")

# Embedding Model Configuration
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "BAAI/bge-large-en-v1.5")
DEVICE = os.getenv("DEVICE", "cuda") # Default to cuda
embedding_model = None # Initialize
EXPECTED_EMBEDDING_DIMENSION = None # Initialize
BREAKPOINT_THRESHOLD_TYPE = 'standard_deviation'
EXPECTED_EMBEDDING_MODEL_ID = "BAAI/bge-large-en-v1.5"
try:
    logger.info(f"Attempting to load Sentence Transformer model '{EMBEDDING_MODEL_NAME}' on device '{DEVICE}'...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)
    EXPECTED_EMBEDDING_DIMENSION = embedding_model.get_sentence_embedding_dimension()
    logger.info(f"‚úÖ Embedding model loaded. Dimension: {EXPECTED_EMBEDDING_DIMENSION}")
except Exception as e:
    logger.error(f"‚ùå Failed to load Sentence Transformer model '{EMBEDDING_MODEL_NAME}': {e}", exc_info=True)
    embedding_model = None # Ensure model is None on error
    # Decide if the app should stop if the model fails to load
    raise RuntimeError(f"Could not load embedding model '{EMBEDDING_MODEL_NAME}'. Application cannot start.") from e

# Text Splitter Configuration
try:
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "900"))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))
    logger.info(f"‚úÖ Text splitter configured with chunk size: {CHUNK_SIZE}, overlap: {CHUNK_OVERLAP}")
except ValueError:
     logger.error("‚ùå Invalid CHUNK_SIZE or CHUNK_OVERLAP in environment variables. Must be integers.")
     CHUNK_SIZE = 900
     CHUNK_OVERLAP = 200
     logger.warning(f"‚ö†Ô∏è Using default chunk size: {CHUNK_SIZE}, overlap: {CHUNK_OVERLAP}")

# OpenSearch Configuration
OPENSEARCH_HOST = os.getenv("OPENSEARCH_HOST")
OPENSEARCH_PORT_STR = os.getenv("OPENSEARCH_PORT", "443")
OPENSEARCH_USER = os.getenv("OPENSEARCH_USER")
OPENSEARCH_PASSWORD = os.getenv("OPENSEARCH_PASSWORD")
OPENSEARCH_INDEX_PREFIX = os.getenv("OPENSEARCH_INDEX_PREFIX", "uservectors")
opensearch = None # Initialize
OPENSEARCH_VECTORS_INDEX_PREFIX = os.getenv('OPENSEARCH_VECTORS_INDEX_PREFIX', 'user_vectors_')
OPENSEARCH_HISTORY_INDEX_PREFIX = os.getenv('OPENSEARCH_HISTORY_INDEX_PREFIX', 'user_history_')

if not all([OPENSEARCH_HOST, OPENSEARCH_PORT_STR, OPENSEARCH_USER, OPENSEARCH_PASSWORD, OPENSEARCH_INDEX_PREFIX]):
     logger.error("‚ùå Missing one or more OpenSearch connection details in environment variables (HOST, PORT, USER, PASSWORD, INDEX_PREFIX).")
else:
    try:
        OPENSEARCH_PORT = int(OPENSEARCH_PORT_STR)
        USE_SSL = OPENSEARCH_PORT == 443
        VERIFY_CERTS = USE_SSL # Default to verifying certs if using SSL

        logger.info(f"Attempting to initialize OpenSearch client for host '{OPENSEARCH_HOST}:{OPENSEARCH_PORT}' (SSL: {USE_SSL})...")
        opensearch = OpenSearch(
            hosts=[{"host": OPENSEARCH_HOST, "port": OPENSEARCH_PORT}],
            http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD), # <<< Use variables from env
            use_ssl=USE_SSL,
            verify_certs=VERIFY_CERTS,
            ssl_assert_hostname=False,
            ssl_show_warn=False,
            connection_class=RequestsHttpConnection,
            timeout=60
        )
        if not opensearch.ping():
            raise ConnectionError("Failed to ping OpenSearch. Check connection details and network access.")
        logger.info("‚úÖ OpenSearch client initialized and connected.")

    except ValueError:
         logger.error(f"‚ùå Invalid OPENSEARCH_PORT: '{OPENSEARCH_PORT_STR}'. Must be an integer.")
         opensearch = None # Ensure client is None on error
    except (ConnectionError, opensearch_exceptions.OpenSearchException) as e:
        logger.error(f"‚ùå Failed to initialize or connect to OpenSearch: {e}", exc_info=True)
        opensearch = None # Ensure client is None on error
    except Exception as e:
         logger.error(f"‚ùå An unexpected error occurred during OpenSearch client initialization: {e}", exc_info=True)
         opensearch = None # Ensure client is None on error


# --- FastAPI App ---
app = FastAPI()

# --- File Processing Functions ---
# (process_pdf, process_txt, process_image, process_docx, etc. - Assume they are defined correctly as in previous versions)
# Definitions omitted here for brevity, but should be included from the previous answer.
# ... (Paste the process_pdf, process_txt, ..., process_csv functions here) ...
def process_pdf(file_path):
    """Extract text and OCR'd text from images in a PDF file."""
    combined_text = []
    try:
        # 1. Extract text directly
        loader = PyPDFLoader(file_path, extract_images=False) # Focus on text first
        documents = loader.load()
        pdf_text = [doc.page_content.strip() for doc in documents if doc.page_content.strip()]
        if pdf_text:
            combined_text.extend(pdf_text)
            logger.debug(f"üìÑ Extracted {len(pdf_text)} text pages from PDF: {os.path.basename(file_path)}")

        # 2. Extract text from images via OCR (can be slow)
        try:
            # Ensure pdf2image doesn't overwhelm memory for large PDFs
            images = convert_from_path(file_path, dpi=200, first_page=1, last_page=20) # Example: Limit pages
            logger.debug(f"üñºÔ∏è Extracted {len(images)} images from PDF for OCR: {os.path.basename(file_path)}")
            ocr_text = []
            for i, image in enumerate(images):
                img_copy = None # Work with a copy if modifying
                try:
                    # Limit image size before OCR if needed
                    # img_copy = image.copy()
                    # img_copy.thumbnail((2000, 2000)) # Example resize
                    text = pytesseract.image_to_string(image, lang='eng', timeout=30).strip() # Specify language, add timeout
                    if text:
                        ocr_text.append(text)
                except RuntimeError as timeout_error:
                    logger.warning(f"‚ö†Ô∏è OCR timed out for an image in {os.path.basename(file_path)}: {timeout_error}")
                except pytesseract.TesseractError as ocr_err:
                     logger.warning(f"‚ö†Ô∏è OCR failed for an image in {os.path.basename(file_path)}: {ocr_err}")
                except Exception as img_err:
                     logger.warning(f"‚ö†Ô∏è Error processing an image for OCR in {os.path.basename(file_path)}: {img_err}")
                finally:
                    # Explicitly clean up image object
                    if 'img_copy' in locals() and img_copy: del img_copy
                    del image # Delete the original page image reference

            if ocr_text:
                combined_text.extend(ocr_text)
                logger.debug(f"üëÅÔ∏è Added {len(ocr_text)} chunks from OCR for PDF: {os.path.basename(file_path)}")

        except Exception as pdf_img_err:
            # Handle pdf2image errors (e.g., Poppler not found, file issues) gracefully
            logger.warning(f"‚ö†Ô∏è Could not extract/process images from PDF {os.path.basename(file_path)}: {pdf_img_err}")

        return combined_text if combined_text else []
    except Exception as e:
        logger.error(f"‚ùå Error processing PDF {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_txt(file_path):
    """Extract text from a plain text file."""
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
        text = None
        detected_encoding = None
        for enc in encodings_to_try:
            try:
                with open(file_path, "r", encoding=enc) as f:
                    text = f.read().strip()
                detected_encoding = enc
                logger.debug(f"üìÑ Read TXT file {os.path.basename(file_path)} using {detected_encoding}")
                break # Success
            except UnicodeDecodeError:
                continue # Try next encoding
            except Exception as e: # Catch other file reading errors
                 logger.error(f"‚ùå Error reading TXT file {os.path.basename(file_path)} with {enc}: {e}")
                 return [] # Stop trying if a non-encoding error occurs

        if text is None:
             logger.warning(f"‚ö†Ô∏è Could not decode TXT file {os.path.basename(file_path)} with tried encodings: {encodings_to_try}.")
             return []

        return [text] if text else []
    except Exception as e:
        logger.error(f"‚ùå Error processing TXT {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_image(file_path):
    """Extract text from an image using OCR."""
    image = None # Initialize
    try:
        image = Image.open(file_path)
        # Optional: Preprocess image (resize, grayscale) if needed for better OCR
        # image = image.convert('L') # Convert to grayscale
        # image.thumbnail((2000, 2000)) # Resize large images
        text = pytesseract.image_to_string(image, lang='eng', timeout=45).strip() # Specify language, add timeout
        logger.debug(f"üñºÔ∏è Extracted text from Image: {os.path.basename(file_path)} (Length: {len(text)})")
        return [text] if text else []
    except RuntimeError as timeout_error:
        logger.warning(f"‚ö†Ô∏è OCR timed out for image {os.path.basename(file_path)}: {timeout_error}")
        return []
    except pytesseract.TesseractNotFoundError:
        logger.error("‚ùå Tesseract is not installed or not in PATH. Cannot process image.")
        return []
    except Exception as e:
        logger.error(f"‚ùå Error processing image {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []
    finally:
        # Clean up image object if it was created
        if image:
            image.close()
            del image

def process_docx(file_path):
    """Extract text from a Word document."""
    try:
        doc = DocxDocument(file_path)
        text_chunks = []
        for para in doc.paragraphs:
            if para.text and para.text.strip():
                text_chunks.append(para.text.strip())
        # Optional: Extract text from tables
        # for table in doc.tables:
        #     # Implementation to extract table text
        #     pass
        combined_text = "\n".join(text_chunks)
        logger.debug(f"üìÑ Extracted text from DOCX: {os.path.basename(file_path)} (Length: {len(combined_text)})")
        return [combined_text] if combined_text else []
    except Exception as e:
        logger.error(f"‚ùå Error processing DOCX {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_pptx(file_path):
    """Extract text from a PowerPoint presentation."""
    try:
        presentation = Presentation(file_path)
        slides_text = []
        for i, slide in enumerate(presentation.slides):
            slide_content = []
            try:
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text and shape.text.strip():
                        slide_content.append(shape.text.strip())
                if slide.has_notes_slide and slide.notes_slide.notes_text_frame:
                    notes_text = slide.notes_slide.notes_text_frame.text.strip()
                    if notes_text:
                         slide_content.append(f"Notes: {notes_text}")
            except Exception as shape_err:
                 logger.warning(f"‚ö†Ô∏è Error processing shapes/notes on slide {i+1} in {os.path.basename(file_path)}: {shape_err}")

            if slide_content:
                slides_text.append(f"Slide {i+1}:\n" + "\n".join(slide_content))
        logger.debug(f"üìÑ Extracted text from {len(slides_text)} slides in PPTX: {os.path.basename(file_path)}")
        return slides_text if slides_text else []
    except Exception as e:
        logger.error(f"‚ùå Error processing PPTX {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_xlsx(file_path):
    """Extract data from an Excel file, sheet by sheet."""
    try:
        excel_data = pd.ExcelFile(file_path)
        text_chunks = []
        for sheet_name in excel_data.sheet_names:
            try:
                df = excel_data.parse(sheet_name, dtype=str).fillna('')
                if not df.empty:
                    sheet_text = f"Sheet: {sheet_name}\n"
                    sheet_text += df.to_csv(index=False, sep='\t')
                    text_chunks.append(sheet_text.strip())
            except Exception as sheet_err:
                logger.warning(f"‚ö†Ô∏è Error processing sheet '{sheet_name}' in {os.path.basename(file_path)}: {sheet_err}")
        logger.debug(f"üìä Extracted text from {len(text_chunks)} sheets in XLSX: {os.path.basename(file_path)}")
        return text_chunks if text_chunks else []
    except Exception as e:
        logger.error(f"‚ùå Error processing XLSX file {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_csv(file_path):
    """Extract data from a CSV file, row by row, handling various encodings and dialects."""
    try:
        text_chunks = []
        encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
        file_processed = False
        detected_encoding = None

        for enc in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=enc, newline='') as csvfile:
                    sample = None
                    try:
                        f_size = os.path.getsize(file_path)
                        if f_size == 0:
                           logger.warning(f"‚ö†Ô∏è CSV file {os.path.basename(file_path)} is empty.")
                           file_processed = True
                           break
                        sample = csvfile.read(min(2048 * 10, f_size))
                        dialect = csv.Sniffer().sniff(sample)
                        csvfile.seek(0)
                        csv_reader = csv.reader(csvfile, dialect)
                        logger.debug(f"üìä Detected CSV dialect for {os.path.basename(file_path)} using {enc}")
                    except (csv.Error, EOFError, ValueError): # Handle sniffing errors
                        logger.warning(f"‚ö†Ô∏è Could not sniff CSV dialect for {os.path.basename(file_path)} using {enc}. Falling back to comma delimiter.")
                        csvfile.seek(0)
                        csv_reader = csv.reader(csvfile)

                    header = next(csv_reader, None)
                    if not header:
                         logger.warning(f"‚ö†Ô∏è CSV file {os.path.basename(file_path)} (encoding {enc}) seems empty or has no header.")
                         file_processed = True
                         break

                    rows_processed = 0
                    for i, row in enumerate(csv_reader):
                         # Skip empty rows
                        if not any(field and field.strip() for field in row):
                            continue
                        row_data = [f"{header[j] if j < len(header) else f'Column_{j+1}'}: {val.strip()}"
                                    for j, val in enumerate(row) if val and val.strip()]
                        if row_data:
                            text_chunks.append(f"Row {i+1}: {', '.join(row_data)}")
                            rows_processed += 1

                detected_encoding = enc
                logger.debug(f"üìä Read {rows_processed} non-empty rows from CSV file {os.path.basename(file_path)} using {detected_encoding}")
                file_processed = True
                break
            except UnicodeDecodeError:
                logger.debug(f"Trying next encoding for {os.path.basename(file_path)}, {enc} failed.")
                continue
            except Exception as read_err:
                logger.error(f"‚ùå Error reading CSV file {os.path.basename(file_path)} with {enc}: {read_err}", exc_info=True)
                file_processed = True
                text_chunks = []
                break

        if not file_processed:
            logger.warning(f"‚ö†Ô∏è Could not decode or process CSV file {os.path.basename(file_path)} with tried encodings: {encodings_to_try}.")

        return text_chunks if text_chunks else []
    except FileNotFoundError:
         logger.error(f"‚ùå CSV file not found: {file_path}")
         return []
    except Exception as e:
        logger.error(f"‚ùå Error processing CSV file {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

# --- Helper Function to Ensure Index Exists ---
def ensure_opensearch_index(
    client: OpenSearch,
    index_name: str,
    index_prefix: str,
    embedding_dimension: int
):
    """Checks if an OpenSearch index exists, creates it if not (WITHOUT 'type' field)."""
    # ... (Input validation for client, index_name, prefix remains the same) ...
    if not client: return False
    if not index_name.startswith(index_prefix + "_"): return False

    # Sanitize index name
    import re as re
    sanitized_index_name = index_name.lower()
    sanitized_index_name = re.sub(r'[^a-z0-9_-]', '_', sanitized_index_name)
    if sanitized_index_name.startswith(('_', '-')):
        sanitized_index_name = index_prefix + sanitized_index_name

    if not sanitized_index_name == index_name:
        logger.warning(f"‚ö†Ô∏è Index name sanitized from '{index_name}' to '{sanitized_index_name}'.")
        index_name = sanitized_index_name

    try:
        if not client.indices.exists(index=index_name):
            logger.info(f"Index '{index_name}' not found. Attempting to create...")
            if not isinstance(embedding_dimension, int) or embedding_dimension <= 0:
                 logger.error(f"‚ùå Invalid embedding dimension for index creation: {embedding_dimension}")
                 return False

            index_body = {
                "settings": { # Standard KNN settings
                    "index": {"knn": True, "knn.algo_param.ef_search": 100}
                },
                "mappings": {
                    "properties": { # Core fields
                        "embedding": {
                            "type": "knn_vector",
                            "dimension": embedding_dimension,
                            "method": {"name": "hnsw", "space_type": "cosinesimil", "engine": "nmslib",
                                       "parameters": {"ef_construction": 128, "m": 24}}
                        },
                        "content": {"type": "text", "analyzer": "standard"},
                        "original_filename": {"type": "keyword"},
                        "user_id": {"type": "keyword"},
                        "s3_key": {"type": "keyword"},
                        "entry_uuid": {"type": "keyword"},
                        "chunk_id": {"type": "integer"},
                        # "type": {"type": "keyword"}, # <<< REMOVED type mapping
                        "timestamp": {"type": "date", "format": "strict_date_optional_time||epoch_millis"}
                    }
                }
            }
            client.indices.create(index=index_name, body=index_body, ignore=[400])
            if client.indices.exists(index=index_name):
                 logger.info(f"‚úÖ Created or ensured OpenSearch index: {index_name}")
                 return True
            else:
                 logger.error(f"‚ùå Failed to create index '{index_name}' despite attempt.")
                 return False
        else:
            return True # Index already exists
    except opensearch_exceptions.RequestError as re:
        if "resource_already_exists_exception" in str(re):
            logger.warning(f"Index '{index_name}' already exists (concurrent creation likely).")
            return True
        else:
             logger.error(f"‚ùå Failed to create index '{index_name}' due to RequestError: {re}", exc_info=True)
             return False
    except opensearch_exceptions.OpenSearchException as e:
        logger.error(f"‚ùå Error checking or creating index '{index_name}': {e}", exc_info=True)
        return False


# --- Core Processing and Indexing Function ---
def file_to_opensearch(
    file_paths: list[str],
    user_id: str,
    s3_key: str,
    entry_uuid: str,
    # file_type: str, # <<< REMOVED file_type parameter
    opensearch_client: OpenSearch,
    opensearch_index_prefix: str,
    embedding_model: SentenceTransformer,
    embedding_dimension: int,
    breakpoint_threshold_type: str,
):
    """
    Processes files, generates embeddings, and stores them in OpenSearch (WITHOUT 'type' field).
    """
    # ... (Input validation for clients, model, user_id remains the same) ...
    if not opensearch_client: return {"status": "error", "message": "OpenSearch client unavailable", "indexed_chunks": 0}
    if not embedding_model: return {"status": "error", "message": "Embedding model unavailable", "indexed_chunks": 0}
    if not isinstance(embedding_dimension, int) or embedding_dimension <= 0: return {"status": "error", "message": "Invalid embedding dimension", "indexed_chunks": 0}
    if not user_id: return {"status": "error", "message": "User ID missing", "indexed_chunks": 0}


    # Determine User-Specific Index Name
    # ... (logic remains the same) ...
    import re as re
    safe_user_id_part = re.sub(r'[^a-z0-9_-]', '_', str(user_id).lower())
    if not safe_user_id_part: return {"status": "error", "message": "Invalid User ID for index creation", "indexed_chunks": 0}
    index_name = f"{opensearch_index_prefix}_{safe_user_id_part}"
    logger.info(f"Targeting OpenSearch index: {index_name} for user: {user_id}")


    # Ensure Index Exists
    # ... (logic remains the same) ...
    if not ensure_opensearch_index(opensearch_client, index_name, opensearch_index_prefix, embedding_dimension):
        return {"status": "error", "message": f"Failed to ensure index '{index_name}' exists", "indexed_chunks": 0}


    # Initialize Text Splitter
    # ... (logic remains the same) ...
    text_splitter = SemanticChunker(
        embedding_model,
        breakpoint_threshold_type=breakpoint_threshold_type
    )


    total_chunks_indexed_session = 0
    files_processed_count = 0
    errors_occurred = False

    for file_path in file_paths:
        # ... (File existence check remains the same) ...
        if not os.path.exists(file_path): continue

        # ... (Extract original filename remains the same) ...
        try: original_filename = os.path.basename(file_path).split('_', 1)[1]
        except IndexError: original_filename = os.path.basename(file_path)

        # logger.info(f"‚öôÔ∏è Processing file: {original_filename} (Entry: {entry_uuid}, Type: {file_type})") # Log removed type
        logger.info(f"‚öôÔ∏è Processing file: {original_filename} (Entry: {entry_uuid})")


        # Select file processing function
        # ... (logic remains the same) ...
        _, file_extension = os.path.splitext(file_path.lower())
        process_func = None
        if file_extension == ".pdf": process_func = process_pdf
        elif file_extension == ".txt": process_func = process_txt
        elif file_extension in (".jpg", ".jpeg", ".png"): process_func = process_image
        elif file_extension == ".docx": process_func = process_docx
        elif file_extension == ".pptx": process_func = process_pptx
        elif file_extension == ".xlsx": process_func = process_xlsx
        elif file_extension == ".csv": process_func = process_csv
        else:
            logger.warning(f"‚ö†Ô∏è Unsupported file type: {original_filename} ({file_extension}), skipping.")
            continue

        try:
            # Process file, combine, split text
            # ... (logic remains the same) ...
            raw_chunks = process_func(file_path) if process_func else []
            if not raw_chunks: continue
            combined_text = "\n\n---\n\n".join(filter(None, [str(chunk).strip() for chunk in raw_chunks]))
            if not combined_text or not combined_text.strip(): continue
            final_texts = text_splitter.split_text(combined_text)
            min_chunk_length = 10
            final_texts = [text for text in final_texts if len(text.strip()) >= min_chunk_length]
            if not final_texts: continue

            # Generate embeddings
            # ... (logic remains the same) ...
            logger.info(f"üìä Extracted {len(raw_chunks)} raw -> {len(final_texts)} final chunks for: {original_filename}")
            logger.info(f"üß† Generating embeddings...")
            embeddings = embedding_model.encode(final_texts, show_progress_bar=False, convert_to_numpy=True)
            logger.info(f"‚úÖ Embeddings generated.")


            # Index chunks into OpenSearch
            logger.info(f"‚úçÔ∏è Indexing {len(final_texts)} chunks into '{index_name}'...")
            indexed_count_for_file = 0
            for i, (text, vector) in enumerate(zip(final_texts, embeddings)):
                # ... (Dimension check remains the same) ...
                if vector.shape[0] != embedding_dimension:
                     logger.error(f"‚ùå Embedding dim mismatch! Expected {embedding_dimension}, got {vector.shape[0]}. Skipping chunk {i}.")
                     errors_occurred = True
                     continue

                doc_id = str(uuid.uuid4())
                doc_body = {
                    "embedding": vector.tolist(),
                    "content": text,
                    "original_filename": original_filename,
                    "user_id": user_id,
                    "s3_key": s3_key,
                    "entry_uuid": entry_uuid,
                    "chunk_id": i,
                    # "type": file_type, # <<< REMOVED type from indexed document
                    "timestamp": datetime.utcnow()
                }

                try:
                    # ... (Indexing call remains the same) ...
                    response = opensearch_client.index(index=index_name, id=doc_id, body=doc_body, refresh=False)
                    if response.get('result') not in ('created', 'updated'): logger.warning(f"‚ö†Ô∏è Unexpected OS index response for chunk {i} (ID: {doc_id}): {response}")
                    indexed_count_for_file += 1
                except opensearch_exceptions.OpenSearchException as os_err:
                    # ... (Error handling remains the same) ...
                    logger.error(f"‚ùå Failed to index chunk {i} (ID: {doc_id}) for {original_filename}: {os_err}", exc_info=True)
                    errors_occurred = True
                    continue

            logger.info(f"‚úÖ Indexed {indexed_count_for_file}/{len(final_texts)} chunks for {original_filename}.")
            total_chunks_indexed_session += indexed_count_for_file
            files_processed_count += 1

        except Exception as e:
            # ... (File processing error handling remains the same) ...
            logger.error(f"‚ùå Unexpected error processing file {original_filename}: {e}", exc_info=True)
            errors_occurred = True
            continue

    # ... (Final logging and return statement remain the same, reflecting overall success/error) ...
    logger.info(f"üèÅ Processing session done for {entry_uuid}. Processed {files_processed_count} files, indexed {total_chunks_indexed_session} chunks into '{index_name}'.")
    return { "status": "error" if errors_occurred else "success", "message": f"Processed {files_processed_count} files. Indexed {total_chunks_indexed_session} chunks." + (" Errors occurred." if errors_occurred else ""), "indexed_chunks": total_chunks_indexed_session, "index_name": index_name }


# --- FastAPI Endpoint ---

class FileProcessPayload(BaseModel): # <<< Model definition updated
    user_id: str
    user_email: str
    original_filename: str
    entry_uuid: str
    s3_key: str
    status: str
    # type: str # <<< REMOVED type field here

# Dependency for API Key Verification
from fastapi import Request

async def verify_api_key(request: Request):
    """Verifies the API key provided in the X-API-KEY header."""
    api_key = request.headers.get("X-API-KEY")

    if not VERIFY_API_KEY_VALUE:
        logger.error("API Key not configured on server. Denying request.")
        raise HTTPException(
            status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="API Key security not configured on server."
        )

    if api_key and api_key == VERIFY_API_KEY_VALUE:
        return True

    logger.warning(f"Invalid API Key received: '{(api_key or '')[:5]}...'")
    raise HTTPException(
        status.HTTP_401_UNAUTHORIZED,
        detail="Invalid or missing API Key"
    )



@app.post("/process-file", status_code=status.HTTP_202_ACCEPTED)
async def process_file_endpoint(
    payload: FileProcessPayload, # <<< Uses the updated model without 'type'
    background_tasks: BackgroundTasks,
    # request: Request, # Can be added back if needed, removed for simplicity
    is_key_valid: bool = Depends(verify_api_key) # Dependency injection for API key
):
    """
    Accepts file processing requests (without 'type'), downloads from S3,
    and queues background task for processing and indexing.
    """
    entry_uuid = payload.entry_uuid
    original_filename = payload.original_filename
    user_id = payload.user_id
    s3_key = payload.s3_key
    # file_type = payload.type # <<< REMOVED this line

    # --- Pre-checks ---
    # ... (Checks for s3_client, opensearch, embedding_model, user_id remain the same) ...
    if not s3_client: raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="S3 service unavailable.")
    if not opensearch: raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Search service unavailable.")
    if not embedding_model or not EXPECTED_EMBEDDING_DIMENSION: raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Embedding service unavailable.")
    if not user_id: raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="user_id is required.")
    # No check needed for file_type anymore


    # --- Define the Background Task ---
    # def background_process_and_index(temp_dir: str, local_file_path: str, type_for_task: str): # <<< Original with type
    def background_process_and_index(temp_dir: str, local_file_path: str): # <<< Updated: Removed type param
        """Function to run in the background (without file_type)."""
        processing_result = {}
        try:
            # logger.info(f"üöÄ Starting background processing: entry={entry_uuid}, user={user_id}, file={original_filename}, type={type_for_task}") # Original log
            logger.info(f"üöÄ Starting background processing: entry={entry_uuid}, user={user_id}, file={original_filename}") # Updated log
            if not os.path.exists(local_file_path):
                 logger.error(f"‚ùå File {local_file_path} missing at start of background task {entry_uuid}.")
                 return

            # Call the main processing function (without file_type)
            processing_result = file_to_opensearch(
                file_paths=[local_file_path],
                user_id=user_id,
                s3_key=s3_key,
                entry_uuid=entry_uuid,
                # file_type=type_for_task, # <<< REMOVED type argument
                opensearch_client=opensearch,
                opensearch_index_prefix=OPENSEARCH_INDEX_PREFIX,
                embedding_model=embedding_model,
                embedding_dimension=EXPECTED_EMBEDDING_DIMENSION,
                breakpoint_threshold_type=BREAKPOINT_THRESHOLD_TYPE
            )
            logger.info(f"‚úÖ Background processing finished: entry={entry_uuid}. Result: {processing_result}")
            # TODO: Update status based on processing_result

        except Exception as e:
            logger.error(f"‚ùå Unexpected error during background task {entry_uuid}: {e}", exc_info=True)
            # TODO: Update status
        finally:
            # Cleanup
            try:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                    logger.info(f"üßπ Cleaned up temp directory: {temp_dir} for {entry_uuid}")
            except Exception as cleanup_err:
                logger.warning(f"‚ö†Ô∏è Failed to clean temp dir {temp_dir} for {entry_uuid}: {cleanup_err}")


    # --- Main Endpoint Logic ---
    temp_dir = tempfile.mkdtemp(prefix=f"s3proc_{entry_uuid}_")
    safe_local_filename = f"{entry_uuid}_{os.path.basename(original_filename)}"
    local_file_path = os.path.join(temp_dir, safe_local_filename)

    try:
        # Step 1: Download from S3
        logger.info(f"üì• Downloading s3://{S3_BUCKET}/{s3_key} to {local_file_path} for {entry_uuid}")
        s3_client.download_file(S3_BUCKET, s3_key, local_file_path)
        logger.info(f"‚úÖ Download complete for {entry_uuid}")

        # Step 2: Queue Background Task (without type)
        # background_tasks.add_task(background_process_and_index, temp_dir, local_file_path, file_type) # <<< Original call
        background_tasks.add_task(background_process_and_index, temp_dir, local_file_path) # <<< Updated call

        # Return Acceptance
        return {
            "message": "Processing accepted and initiated in background.",
            "entry_uuid": entry_uuid,
            "filename": original_filename,
            "user_id": user_id
            # "type": file_type # <<< REMOVED type from response
        }

    except ClientError as s3_err:
         # ... (S3 error handling remains the same) ...
         error_code = s3_err.response.get("Error", {}).get("Code")
         log_exc_info = error_code not in ['404', 'NoSuchKey', '403', 'AccessDenied']
         logger.error(f"‚ùå S3 Download failed for {s3_key} ({error_code}): {s3_err}", exc_info=log_exc_info)
         if error_code in ('404', 'NoSuchKey'): detail = f"File not found in S3: s3://{S3_BUCKET}/{s3_key}"; status_code = status.HTTP_404_NOT_FOUND
         elif error_code in ('403', 'AccessDenied'): detail = f"Access denied for S3 key: {s3_key}"; status_code = status.HTTP_403_FORBIDDEN
         else: detail = f"Error downloading from S3: {s3_err}"; status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
         shutil.rmtree(temp_dir, ignore_errors=True)
         raise HTTPException(status_code=status_code, detail=detail)
    except Exception as e:
        # ... (General error handling remains the same) ...
        logger.error(f"‚ùå Error initiating processing for {entry_uuid}: {e}", exc_info=True)
        shutil.rmtree(temp_dir, ignore_errors=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to initiate processing: {str(e)}")
    
    
def create_history_index_if_not_exists(client: OpenSearch, index_name: str):
    """Creates the OpenSearch index for chat history storage if it doesn't exist."""
    if not client: logger.error("OpenSearch client unavailable."); return False
    try:
        if not client.indices.exists(index=index_name):
             logger.info(f"History index '{index_name}' does not exist. Creating...")
             mapping = {
                 "mappings": {
                     "properties": {
                         "user_id": {"type": "keyword"}, "session_id": {"type": "keyword"},
                         "human_message": {"type": "text"}, "ai_message": {"type": "text"},
                         "timestamp": {"type": "date", "format": "strict_date_optional_time||epoch_millis"} # Flexible date format
                     }
                 }
             }
             create_response = client.indices.create(index=index_name, body=mapping, ignore=[400, 404])
             logger.info(f"Index creation response for '{index_name}': {create_response}")
             if client.indices.exists(index=index_name): logger.info(f"History index '{index_name}' created."); return True
             else: logger.error(f"History index '{index_name}' creation failed verification."); return False
        else: return True # Index already exists
    except Exception as e: logger.error(f"Failed create/check history index '{index_name}': {e}", exc_info=True); return False

from fastapi import APIRouter, HTTPException, Depends, status
from typing import List

router = APIRouter()

@router.post("/vectors/search", response_model=SearchResponse)
async def search_documents_endpoint(
    payload: SearchRequestPayload,
    is_key_valid: bool = Depends(verify_api_key)
):
    """
    Performs a semantic + keyword hybrid search for a given query within a specific user's
    document index in OpenSearch.
    """
    user_id = payload.user_id
    query = payload.query
    top_k = payload.top_k

    logger.info(f"üîç Received search request: user_id='{user_id}', top_k={top_k}, query='{query[:50]}...'")

    # Pre-checks
    if not opensearch:
        logger.error(f"Search rejected for user {user_id}: OpenSearch client not available.")
        raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Search service unavailable.")
    if not embedding_model:
        logger.error(f"Search rejected for user {user_id}: Embedding model not ready.")
        raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Embedding service unavailable.")
    if not user_id or not query:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="user_id and query are required.")

    # Determine per-user index name
    import re
    safe_user_id = re.sub(r'[^a-z0-9_-]', '_', str(user_id).lower()) or None
    if not safe_user_id:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="Invalid user_id format.")
    index_name = f"{OPENSEARCH_INDEX_PREFIX}_{safe_user_id}"
    if not opensearch.indices.exists(index=index_name):
        return SearchResponse(results=[], index_queried=index_name)

    # Compute embedding vector for neural part
    try:
        query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()
    except Exception as e:
        logger.error(f"Embedding error: {e}")
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to embed query.")

    # Build hybrid search body
    search_body = {
        "_source": {
            "exclude": ["passage_embedding"]  # Adjust field names as needed
        },
        "query": {
            "hybrid": {
                "queries": [
                    {
                        "match": {
                            "content": {
                                "query": query,
                                "operator": "and"  # Optional: tighten match behavior
                            }
                        }
                    },
                    {
                        "neural": {
                            "embedding": {
                                "query_vector": query_embedding,
                                "model_id": EXPECTED_EMBEDDING_MODEL_ID,
                                "k": top_k
                            }
                        }
                    }
                ],
                "size": top_k  # total results to return
            }
        }
    }

    # Execute search
    try:
        logger.info(f"Executing hybrid search on {index_name}")
        response = opensearch.search(index=index_name, body=search_body)
    except Exception as e:
        logger.error(f"Search execution failed: {e}")
        raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Search execution error.")

    # Parse hits
    hits = response.get("hits", {}).get("hits", [])
    results = []
    for hit in hits:
        src = hit.get("_source", {})
        score = hit.get("_score", 0.0)
        if all(k in src for k in ("chunk_id", "content", "original_filename", "s3_key", "entry_uuid")):
            results.append(SearchResult(
                chunk_id=src["chunk_id"],
                content=src["content"],
                original_filename=src["original_filename"],
                s3_key=src["s3_key"],
                entry_uuid=src["entry_uuid"],
                score=score
            ))
    logger.info(f"Returning {len(results)} results")
    return SearchResponse(results=results, index_queried=index_name)

def _get_safe_user_id(user_id: str) -> str:
    """Converts user ID to a safe format for index names."""
    return str(user_id).lower().replace('-', '_').replace('.', '_').replace('@','_') # Replace common problematic chars

def get_vector_index_name(user_id: str) -> str:
    """Generates the index name for user document vectors."""
    safe_user_id = _get_safe_user_id(user_id)
    return f"{OPENSEARCH_VECTORS_INDEX_PREFIX}{safe_user_id}"

def get_history_index_name(user_id: str) -> str:
    """Generates the index name for user chat history."""
    safe_user_id = _get_safe_user_id(user_id)
    return f"{OPENSEARCH_HISTORY_INDEX_PREFIX}{safe_user_id}"

@app.post("/chat/history", status_code=status.HTTP_201_CREATED)
async def store_chat_history( payload: ChatHistoryPayload, is_key_valid: bool = Depends(verify_api_key) ):
    """Stores a user message and/or AI response in the user-specific history index."""
    # User ID for isolation is taken directly from the payload
    user_id = payload.user_id
    logger.info(f"Received /chat/history request for user={user_id}, session={payload.session_id}")

    if not opensearch: logger.error("OS client unavailable."); raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, "Search unavailable.")

    index_name = get_history_index_name(user_id) # User-specific index
    if not create_history_index_if_not_exists(opensearch, index_name):
        logger.error(f"Failed ensure history index '{index_name}' exists.")
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed prepare history storage.")

    doc_id = str(uuid.uuid4())
    doc_body = {
        "user_id": user_id, # Store user_id for potential checks (though index isolates)
        "session_id": payload.session_id,
        "human_message": payload.human_message,
        "ai_message": payload.ai_message,
        "timestamp": datetime.now(timezone.utc)
    }

    try:
        response = opensearch.index( index=index_name, id=doc_id, body=doc_body, refresh="wait_for" )
        logger.info(f"Stored history doc={doc_id} in index='{index_name}'. Resp: {response.get('result')}")
        return {"message": "Chat history stored.", "doc_id": doc_id}
    except Exception as e:
        logger.error(f"Failed index history doc={doc_id} into '{index_name}': {e}", exc_info=True)
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed store chat history.")

# --- Endpoint 3: Retrieve Full Chat History ---
@app.get("/chat/history/retrieve", response_model=List[HistoryEntryResponse])
async def retrieve_chat_history(
    user_id: str = Query(..., description="The ID of the user whose history to retrieve."),
    session_id: Optional[str] = Query(None, description="Optional session ID to filter history."),
    limit: int = Query(100, ge=1, le=1000, description="Max entries."),
    sort_order: str = Query("asc", pattern="^(asc|desc)$", description="'asc' or 'desc' by timestamp."),
    is_key_valid: bool = Depends(verify_api_key)
):
    """Retrieves stored chat history entries for the specified user, ensuring isolation."""
    # User ID for isolation is taken directly from the query parameter
    logger.info(f"Received /chat/history/retrieve request user={user_id}, session={session_id}, limit={limit}, sort={sort_order}")

    if not opensearch: logger.error("OS client unavailable."); raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, "Search unavailable.")

    index_name = get_history_index_name(user_id) # User-specific index

    try:
        if not opensearch.indices.exists(index=index_name):
            logger.info(f"History index '{index_name}' not found for user {user_id}. Returning [].")
            return []

        query_body = {
            "size": limit,
            "query": { "bool": { "filter": [ {"term": {"user_id": user_id}} ] } }, # Filter by user_id just in case, though index isolates
            "sort": [ {"timestamp": {"order": sort_order}} ],
            "_source": True
        }
        if session_id:
            query_body["query"]["bool"]["filter"].append({"term": {"session_id": session_id}})
            logger.debug(f"Filtering history retrieval by session_id: {session_id}")

        response = opensearch.search( index=index_name, body=query_body )
        hits = response.get('hits', {}).get('hits', [])
        # Parse into Pydantic model for validation and consistent output
        history_entries = [HistoryEntryResponse(**hit['_source']) for hit in hits]
        logger.info(f"Retrieved {len(history_entries)} history entries for user {user_id} from '{index_name}'.")
        return history_entries

    except NotFoundError: logger.info(f"History index '{index_name}' not found user={user_id}. Return []."); return []
    except Exception as e: logger.error(f"Error retrieve history '{index_name}' user={user_id}: {e}", exc_info=True); raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed retrieve chat history.")

@app.get("/health")
async def health_check():
    # ... (health check code remains the same) ...
    opensearch_ok = False
    if opensearch:
        try: opensearch_ok = opensearch.ping()
        except Exception: opensearch_ok = False
    embedding_model_ok = embedding_model is not None
    s3_client_ok = s3_client is not None
    status_code = status.HTTP_200_OK if opensearch_ok and embedding_model_ok and s3_client_ok else status.HTTP_503_SERVICE_UNAVAILABLE
    return {
        "status": "ok" if status_code == status.HTTP_200_OK else "error",
        "dependencies": {
            "opensearch_connection": "ok" if opensearch_ok else "error",
            "embedding_model_loaded": "ok" if embedding_model_ok else "error",
            "s3_client_initialized": "ok" if s3_client_ok else "error"
        }
    }, status_code

# --- Example Run Command ---
if __name__ == "__main__":
    import uvicorn
    logger.info("Starting FastAPI server...")
    # ... (Readiness checks remain the same) ...
    if not s3_client: logger.critical("‚ùå S3 Client failed initialization.")
    if not opensearch: logger.critical("‚ùå OpenSearch Client failed initialization.")
    if not embedding_model: logger.critical("‚ùå Embedding Model failed initialization.")

    uvicorn.run(app, host="0.0.0.0", port=9001)