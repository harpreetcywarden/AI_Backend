import os
import csv
import base64
import uuid 
import time
from datetime import datetime, timezone
import pytesseract
import pandas as pd
import tempfile
import shutil
import logging
import boto3
from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ClientError
from pydantic import BaseModel, Field
from typing import List, Optional
from dotenv import load_dotenv
from datetime import datetime # Optional for timestamping

from fastapi import FastAPI, Depends, HTTPException, status, BackgroundTasks, Request, Query 
from pydantic import BaseModel, Field, model_validator
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from sentence_transformers import SentenceTransformer
from opensearchpy import OpenSearch, RequestsHttpConnection, exceptions as opensearch_exceptions
from opensearchpy.exceptions import NotFoundError
from docx import Document as DocxDocument
from pptx import Presentation
from PIL import Image
from pdf2image import convert_from_path

# --- Load Environment Variables ---
load_dotenv()
print("Attempting to load environment variables...")

# --- Configuration from Environment Variables ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# API Key
VERIFY_API_KEY_VALUE = os.getenv("MY_PROCESSING_API_KEY")
if not VERIFY_API_KEY_VALUE:
    logger.warning("⚠️ MY_PROCESSING_API_KEY not found in environment variables. API key check will fail.")
    
    
class SearchRequestPayload(BaseModel):
    user_id: str
    query: str
    top_k: int = Field(default=5, gt=0, le=50) # Default to 5 results, min 1, max 50 (adjustable)
    # Optional: Add a minimum score threshold if needed
    # min_score: Optional[float] = Field(default=None, ge=0.0, le=1.0)

class HistoryEntryResponse(BaseModel): # For retrieving history
    user_id: str
    session_id: Optional[str] = None
    human_message: Optional[str] = None
    ai_message: Optional[str] = None
    timestamp: datetime

class HistoryEntryResponse(BaseModel): # For retrieving history
    user_id: str
    session_id: Optional[str] = None
    human_message: Optional[str] = None
    ai_message: Optional[str] = None
    timestamp: datetime

class SearchResult(BaseModel):
    chunk_id: int
    content: str
    original_filename: str
    s3_key: str  # Include s3_key for context
    entry_uuid: str # Include entry_uuid for context
    score: float # Similarity score from OpenSearch

class SearchResponse(BaseModel):
    results: List[SearchResult]
    index_queried: str # Inform which index was used

class ProcessRequestPayload(BaseModel):
    user_id: str
    user_email: Optional[str] = None
    original_filename: str
    entry_uuid: str
    s3_key: str
    status: str

class VectorDeletePayload(BaseModel):
 user_id: str = Field(..., description="The ID of the user whose vectors should be deleted.")
 entry_uuid: str = Field(..., description="The UUID of the entry whose vectors should be deleted.")


class ChatHistoryPayload(BaseModel): # For storing history
    user_id: str = Field(..., description="The ID of the user initiating the chat turn.")
    session_id: Optional[str] = Field(None, description="An identifier for the chat session.")
    human_message: Optional[str] = Field(None, description="The message sent by the human user.")
    ai_message: Optional[str] = Field(None, description="The message generated by the AI.")

    # Ensure at least one message is present
    @model_validator(mode='after')
    def check_messages_present(cls, values):
        if not values.human_message and not values.ai_message:
            raise ValueError('At least one of human_message or ai_message must be provided')
        return values


# S3 Configuration
S3_BUCKET = os.getenv("AWS_STORAGE_BUCKET_NAME")
S3_REGION = os.getenv("AWS_S3_REGION_NAME")
s3_client = None # Initialize as None
if not S3_BUCKET or not S3_REGION:
    logger.error("❌ AWS_STORAGE_BUCKET_NAME or AWS_S3_REGION_NAME not set in environment variables.")
else:
    try:
        logger.info(f"Attempting to initialize S3 client for bucket '{S3_BUCKET}' in region '{S3_REGION}'...")
        s3_client = boto3.client('s3', region_name=S3_REGION)
        s3_client.head_bucket(Bucket=S3_BUCKET)
        logger.info(f"✅ S3 client initialized and bucket '{S3_BUCKET}' accessible.")
    except (NoCredentialsError, PartialCredentialsError) as cred_err:
        logger.error(f"❌ AWS credentials error: {cred_err}")
        s3_client = None # Ensure client is None on error
    except ClientError as e:
        error_code = e.response.get("Error", {}).get("Code")
        logger.error(f"❌ S3 ClientError ({error_code}): {e}", exc_info=(error_code not in ['NoSuchBucket', 'AccessDenied']))
        s3_client = None # Ensure client is None on error
    except Exception as e:
        logger.error(f"❌ An unexpected error occurred during S3 client initialization: {e}", exc_info=True)
        s3_client = None # Ensure client is None on error


# Tesseract Configuration
try:
    pytesseract.pytesseract.get_tesseract_version()
    logger.info(f"✅ Tesseract found at: {pytesseract.pytesseract.tesseract_cmd}")
except pytesseract.TesseractNotFoundError:
    logger.error("❌ Tesseract is not installed or not found in PATH. OCR functionality will fail.")

# Embedding Model Configuration
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "all-MiniLM-L6-v2")
DEVICE = os.getenv("DEVICE", "cuda") # Default to cuda
embedding_model = None # Initialize
EXPECTED_EMBEDDING_DIMENSION = None # Initialize
try:
    logger.info(f"Attempting to load Sentence Transformer model '{EMBEDDING_MODEL_NAME}' on device '{DEVICE}'...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)
    EXPECTED_EMBEDDING_DIMENSION = embedding_model.get_sentence_embedding_dimension()
    logger.info(f"✅ Embedding model loaded. Dimension: {EXPECTED_EMBEDDING_DIMENSION}")
except Exception as e:
    logger.error(f"❌ Failed to load Sentence Transformer model '{EMBEDDING_MODEL_NAME}': {e}", exc_info=True)
    embedding_model = None # Ensure model is None on error
    # Decide if the app should stop if the model fails to load
    raise RuntimeError(f"Could not load embedding model '{EMBEDDING_MODEL_NAME}'. Application cannot start.") from e

# Text Splitter Configuration
try:
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "900"))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))
    logger.info(f"✅ Text splitter configured with chunk size: {CHUNK_SIZE}, overlap: {CHUNK_OVERLAP}")
except ValueError:
     logger.error("❌ Invalid CHUNK_SIZE or CHUNK_OVERLAP in environment variables. Must be integers.")
     CHUNK_SIZE = 900
     CHUNK_OVERLAP = 200
     logger.warning(f"⚠️ Using default chunk size: {CHUNK_SIZE}, overlap: {CHUNK_OVERLAP}")

# OpenSearch Configuration
OPENSEARCH_HOST = os.getenv("OPENSEARCH_HOST")
OPENSEARCH_PORT_STR = os.getenv("OPENSEARCH_PORT", "443")
OPENSEARCH_USER = os.getenv("OPENSEARCH_USER")
OPENSEARCH_PASSWORD = os.getenv("OPENSEARCH_PASSWORD")
OPENSEARCH_INDEX_PREFIX = os.getenv("OPENSEARCH_INDEX_PREFIX", "uservectors")
opensearch = None # Initialize
OPENSEARCH_VECTORS_INDEX_PREFIX = os.getenv('OPENSEARCH_VECTORS_INDEX_PREFIX', 'user_vectors_')
OPENSEARCH_HISTORY_INDEX_PREFIX = os.getenv('OPENSEARCH_HISTORY_INDEX_PREFIX', 'user_history_')

if not all([OPENSEARCH_HOST, OPENSEARCH_PORT_STR, OPENSEARCH_USER, OPENSEARCH_PASSWORD, OPENSEARCH_INDEX_PREFIX]):
     logger.error("❌ Missing one or more OpenSearch connection details in environment variables (HOST, PORT, USER, PASSWORD, INDEX_PREFIX).")
else:
    try:
        OPENSEARCH_PORT = int(OPENSEARCH_PORT_STR)
        USE_SSL = OPENSEARCH_PORT == 443
        VERIFY_CERTS = USE_SSL # Default to verifying certs if using SSL

        logger.info(f"Attempting to initialize OpenSearch client for host '{OPENSEARCH_HOST}:{OPENSEARCH_PORT}' (SSL: {USE_SSL})...")
        opensearch = OpenSearch(
            hosts=[{"host": OPENSEARCH_HOST, "port": OPENSEARCH_PORT}],
            http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD), # <<< Use variables from env
            use_ssl=USE_SSL,
            verify_certs=VERIFY_CERTS,
            ssl_assert_hostname=False,
            ssl_show_warn=False,
            connection_class=RequestsHttpConnection,
            timeout=60
        )
        if not opensearch.ping():
            raise ConnectionError("Failed to ping OpenSearch. Check connection details and network access.")
        logger.info("✅ OpenSearch client initialized and connected.")

    except ValueError:
         logger.error(f"❌ Invalid OPENSEARCH_PORT: '{OPENSEARCH_PORT_STR}'. Must be an integer.")
         opensearch = None # Ensure client is None on error
    except (ConnectionError, opensearch_exceptions.OpenSearchException) as e:
        logger.error(f"❌ Failed to initialize or connect to OpenSearch: {e}", exc_info=True)
        opensearch = None # Ensure client is None on error
    except Exception as e:
         logger.error(f"❌ An unexpected error occurred during OpenSearch client initialization: {e}", exc_info=True)
         opensearch = None # Ensure client is None on error


# --- FastAPI App ---
app = FastAPI()

# --- File Processing Functions ---
def process_pdf(file_path):
    """Extract text and images from a PDF file."""
    try:
        # Step 1: Extract text from PDF pages
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        pdf_text = [doc.page_content.strip() for doc in documents if doc.page_content.strip()]

        # Step 2: Extract images from PDF pages
        images = convert_from_path(file_path)  # Convert PDF pages to images
        ocr_text = []
        for image in images:
            text = pytesseract.image_to_string(image).strip()  # Perform OCR on each image
            if text:
                ocr_text.append(text)

        # Step 3: Combine text from PDF pages and OCR results
        combined_text = pdf_text + ocr_text
        return combined_text if combined_text else []  # Return combined text
    except Exception as e:
        print(f"⚠️ Error processing PDF {file_path}: {str(e)}")
        return []

def process_txt(file_path):
    """Extract text from a plain text file."""
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
        text = None
        detected_encoding = None
        for enc in encodings_to_try:
            try:
                with open(file_path, "r", encoding=enc) as f:
                    text = f.read().strip()
                detected_encoding = enc
                logger.debug(f"📄 Read TXT file {os.path.basename(file_path)} using {detected_encoding}")
                break # Success
            except UnicodeDecodeError:
                continue # Try next encoding
            except Exception as e: # Catch other file reading errors
                 logger.error(f"❌ Error reading TXT file {os.path.basename(file_path)} with {enc}: {e}")
                 return [] # Stop trying if a non-encoding error occurs

        if text is None:
             logger.warning(f"⚠️ Could not decode TXT file {os.path.basename(file_path)} with tried encodings: {encodings_to_try}.")
             return []

        return [text] if text else []
    except Exception as e:
        logger.error(f"❌ Error processing TXT {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_image(file_path):
    """Extract text from an image using OCR."""
    image = None # Initialize
    try:
        image = Image.open(file_path)
        # image = image.convert('L') # Convert to grayscale
        # image.thumbnail((2000, 2000)) # Resize large images
        text = pytesseract.image_to_string(image, lang='eng', timeout=45).strip() # Specify language, add timeout
        logger.debug(f"🖼️ Extracted text from Image: {os.path.basename(file_path)} (Length: {len(text)})")
        return [text] if text else []
    except RuntimeError as timeout_error:
        logger.warning(f"⚠️ OCR timed out for image {os.path.basename(file_path)}: {timeout_error}")
        return []
    except pytesseract.TesseractNotFoundError:
        logger.error("❌ Tesseract is not installed or not in PATH. Cannot process image.")
        return []
    except Exception as e:
        logger.error(f"❌ Error processing image {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []
    finally:
        if image:
            image.close()
            del image

def process_docx(file_path):
    """Extract text from a Word document."""
    try:
        doc = DocxDocument(file_path)
        text_chunks = []
        for para in doc.paragraphs:
            if para.text and para.text.strip():
                text_chunks.append(para.text.strip())
        # Optional: Extract text from tables
        # for table in doc.tables:
        #     # Implementation to extract table text
        #     pass
        combined_text = "\n".join(text_chunks)
        logger.debug(f"📄 Extracted text from DOCX: {os.path.basename(file_path)} (Length: {len(combined_text)})")
        return [combined_text] if combined_text else []
    except Exception as e:
        logger.error(f"❌ Error processing DOCX {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_pptx(file_path):
    """Extract text from a PowerPoint presentation."""
    try:
        presentation = Presentation(file_path)
        slides_text = []
        for i, slide in enumerate(presentation.slides):
            slide_content = []
            try:
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text and shape.text.strip():
                        slide_content.append(shape.text.strip())
                if slide.has_notes_slide and slide.notes_slide.notes_text_frame:
                    notes_text = slide.notes_slide.notes_text_frame.text.strip()
                    if notes_text:
                         slide_content.append(f"Notes: {notes_text}")
            except Exception as shape_err:
                 logger.warning(f"⚠️ Error processing shapes/notes on slide {i+1} in {os.path.basename(file_path)}: {shape_err}")

            if slide_content:
                slides_text.append(f"Slide {i+1}:\n" + "\n".join(slide_content))
        logger.debug(f"📄 Extracted text from {len(slides_text)} slides in PPTX: {os.path.basename(file_path)}")
        return slides_text if slides_text else []
    except Exception as e:
        logger.error(f"❌ Error processing PPTX {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_xlsx(file_path):
    """Extract data from an Excel file, sheet by sheet."""
    try:
        excel_data = pd.ExcelFile(file_path)
        text_chunks = []
        for sheet_name in excel_data.sheet_names:
            try:
                df = excel_data.parse(sheet_name, dtype=str).fillna('')
                if not df.empty:
                    sheet_text = f"Sheet: {sheet_name}\n"
                    sheet_text += df.to_csv(index=False, sep='\t')
                    text_chunks.append(sheet_text.strip())
            except Exception as sheet_err:
                logger.warning(f"⚠️ Error processing sheet '{sheet_name}' in {os.path.basename(file_path)}: {sheet_err}")
        logger.debug(f"📊 Extracted text from {len(text_chunks)} sheets in XLSX: {os.path.basename(file_path)}")
        return text_chunks if text_chunks else []
    except Exception as e:
        logger.error(f"❌ Error processing XLSX file {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

def process_csv(file_path):
    """Extract data from a CSV file, row by row, handling various encodings and dialects."""
    try:
        text_chunks = []
        encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
        file_processed = False
        detected_encoding = None

        for enc in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=enc, newline='') as csvfile:
                    sample = None
                    try:
                        f_size = os.path.getsize(file_path)
                        if f_size == 0:
                           logger.warning(f"⚠️ CSV file {os.path.basename(file_path)} is empty.")
                           file_processed = True
                           break
                        sample = csvfile.read(min(2048 * 10, f_size))
                        dialect = csv.Sniffer().sniff(sample)
                        csvfile.seek(0)
                        csv_reader = csv.reader(csvfile, dialect)
                        logger.debug(f"📊 Detected CSV dialect for {os.path.basename(file_path)} using {enc}")
                    except (csv.Error, EOFError, ValueError): # Handle sniffing errors
                        logger.warning(f"⚠️ Could not sniff CSV dialect for {os.path.basename(file_path)} using {enc}. Falling back to comma delimiter.")
                        csvfile.seek(0)
                        csv_reader = csv.reader(csvfile)

                    header = next(csv_reader, None)
                    if not header:
                         logger.warning(f"⚠️ CSV file {os.path.basename(file_path)} (encoding {enc}) seems empty or has no header.")
                         file_processed = True
                         break

                    rows_processed = 0
                    for i, row in enumerate(csv_reader):
                         # Skip empty rows
                        if not any(field and field.strip() for field in row):
                            continue
                        row_data = [f"{header[j] if j < len(header) else f'Column_{j+1}'}: {val.strip()}"
                                    for j, val in enumerate(row) if val and val.strip()]
                        if row_data:
                            text_chunks.append(f"Row {i+1}: {', '.join(row_data)}")
                            rows_processed += 1

                detected_encoding = enc
                logger.debug(f"📊 Read {rows_processed} non-empty rows from CSV file {os.path.basename(file_path)} using {detected_encoding}")
                file_processed = True
                break
            except UnicodeDecodeError:
                logger.debug(f"Trying next encoding for {os.path.basename(file_path)}, {enc} failed.")
                continue
            except Exception as read_err:
                logger.error(f"❌ Error reading CSV file {os.path.basename(file_path)} with {enc}: {read_err}", exc_info=True)
                file_processed = True
                text_chunks = []
                break

        if not file_processed:
            logger.warning(f"⚠️ Could not decode or process CSV file {os.path.basename(file_path)} with tried encodings: {encodings_to_try}.")

        return text_chunks if text_chunks else []
    except FileNotFoundError:
         logger.error(f"❌ CSV file not found: {file_path}")
         return []
    except Exception as e:
        logger.error(f"❌ Error processing CSV file {os.path.basename(file_path)}: {str(e)}", exc_info=True)
        return []

# --- Helper Function to Ensure Index Exists ---
def ensure_opensearch_index(
    client: OpenSearch,
    index_name: str,
    index_prefix: str,
    embedding_dimension: int
):
    """Checks if an OpenSearch index exists, creates it if not (WITHOUT 'type' field)."""
    # ... (Input validation for client, index_name, prefix remains the same) ...
    if not client: return False
    if not index_name.startswith(index_prefix + "_"): return False

    # Sanitize index name
    import re as re
    sanitized_index_name = index_name.lower()
    sanitized_index_name = re.sub(r'[^a-z0-9_-]', '_', sanitized_index_name)
    if sanitized_index_name.startswith(('_', '-')):
        sanitized_index_name = index_prefix + sanitized_index_name

    if not sanitized_index_name == index_name:
        logger.warning(f"⚠️ Index name sanitized from '{index_name}' to '{sanitized_index_name}'.")
        index_name = sanitized_index_name

    try:
        if not client.indices.exists(index=index_name):
            logger.info(f"Index '{index_name}' not found. Attempting to create...")
            if not isinstance(embedding_dimension, int) or embedding_dimension <= 0:
                 logger.error(f"❌ Invalid embedding dimension for index creation: {embedding_dimension}")
                 return False

            index_body = {
                "settings": { # Standard KNN settings
                    "index": {"knn": True, "knn.algo_param.ef_search": 100}
                },
                "mappings": {
                    "properties": { # Core fields
                        "embedding": {
                            "type": "knn_vector",
                            "dimension": embedding_dimension,
                            "method": {"name": "hnsw", "space_type": "cosinesimil", "engine": "nmslib",
                                       "parameters": {"ef_construction": 128, "m": 24}}
                        },
                        "content": {"type": "text", "analyzer": "standard"},
                        "original_filename": {"type": "keyword"},
                        "user_id": {"type": "keyword"},
                        "s3_key": {"type": "keyword"},
                        "entry_uuid": {"type": "keyword"},
                        "chunk_id": {"type": "integer"},
                        # "type": {"type": "keyword"}, # <<< REMOVED type mapping
                        "timestamp": {"type": "date", "format": "strict_date_optional_time||epoch_millis"}
                    }
                }
            }
            client.indices.create(index=index_name, body=index_body, ignore=[400])
            if client.indices.exists(index=index_name):
                 logger.info(f"✅ Created or ensured OpenSearch index: {index_name}")
                 return True
            else:
                 logger.error(f"❌ Failed to create index '{index_name}' despite attempt.")
                 return False
        else:
            return True # Index already exists
    except opensearch_exceptions.RequestError as re:
        if "resource_already_exists_exception" in str(re):
            logger.warning(f"Index '{index_name}' already exists (concurrent creation likely).")
            return True
        else:
             logger.error(f"❌ Failed to create index '{index_name}' due to RequestError: {re}", exc_info=True)
             return False
    except opensearch_exceptions.OpenSearchException as e:
        logger.error(f"❌ Error checking or creating index '{index_name}': {e}", exc_info=True)
        return False


# --- Core Processing and Indexing Function ---
def file_to_opensearch(
    file_paths: list[str],
    user_id: str,
    s3_key: str,
    entry_uuid: str,
    # file_type: str, # <<< REMOVED file_type parameter
    opensearch_client: OpenSearch,
    opensearch_index_prefix: str,
    embedding_model: SentenceTransformer,
    embedding_dimension: int,
    chunk_size: int,
    chunk_overlap: int
):
    """
    Processes files, generates embeddings, and stores them in OpenSearch (WITHOUT 'type' field).
    """
    # ... (Input validation for clients, model, user_id remains the same) ...
    if not opensearch_client: return {"status": "error", "message": "OpenSearch client unavailable", "indexed_chunks": 0}
    if not embedding_model: return {"status": "error", "message": "Embedding model unavailable", "indexed_chunks": 0}
    if not isinstance(embedding_dimension, int) or embedding_dimension <= 0: return {"status": "error", "message": "Invalid embedding dimension", "indexed_chunks": 0}
    if not user_id: return {"status": "error", "message": "User ID missing", "indexed_chunks": 0}


    # Determine User-Specific Index Name
    import re as re
    safe_user_id_part = re.sub(r'[^a-z0-9_-]', '_', str(user_id).lower())
    if not safe_user_id_part: return {"status": "error", "message": "Invalid User ID for index creation", "indexed_chunks": 0}
    index_name = f"{opensearch_index_prefix}_{safe_user_id_part}"
    logger.info(f"Targeting OpenSearch index: {index_name} for user: {user_id}")

    if not ensure_opensearch_index(opensearch_client, index_name, opensearch_index_prefix, embedding_dimension):
        return {"status": "error", "message": f"Failed to ensure index '{index_name}' exists", "indexed_chunks": 0}

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len,
        is_separator_regex=False, separators=["\n\n", "\n", ". ", " ", ""]
    )


    total_chunks_indexed_session = 0
    files_processed_count = 0
    errors_occurred = False

    for file_path in file_paths:
        if not os.path.exists(file_path): continue

        try: original_filename = os.path.basename(file_path).split('_', 1)[1]
        except IndexError: original_filename = os.path.basename(file_path)

        logger.info(f"⚙️ Processing file: {original_filename} (Entry: {entry_uuid})")


        # Select file processing function
        _, file_extension = os.path.splitext(file_path.lower())
        process_func = None
        if file_extension == ".pdf": process_func = process_pdf
        elif file_extension == ".txt": process_func = process_txt
        elif file_extension in (".jpg", ".jpeg", ".png"): process_func = process_image
        elif file_extension == ".docx": process_func = process_docx
        elif file_extension == ".pptx": process_func = process_pptx
        elif file_extension == ".xlsx": process_func = process_xlsx
        elif file_extension == ".csv": process_func = process_csv
        else:
            logger.warning(f"⚠️ Unsupported file type: {original_filename} ({file_extension}), skipping.")
            continue

        try:
            # Process file, combine, split text
            raw_chunks = process_func(file_path) if process_func else []
            if not raw_chunks: continue
            combined_text = "\n\n---\n\n".join(filter(None, [str(chunk).strip() for chunk in raw_chunks]))
            if not combined_text or not combined_text.strip(): continue
            final_texts = text_splitter.split_text(combined_text)
            min_chunk_length = 10
            final_texts = [text for text in final_texts if len(text.strip()) >= min_chunk_length]
            if not final_texts: continue

            # Generate embeddings
            logger.info(f"📊 Extracted {len(raw_chunks)} raw -> {len(final_texts)} final chunks for: {original_filename}")
            logger.info(f"🧠 Generating embeddings...")
            embeddings = embedding_model.encode(final_texts, show_progress_bar=False, convert_to_numpy=True)
            logger.info(f"✅ Embeddings generated.")


            # Index chunks into OpenSearch
            logger.info(f"✍️ Indexing {len(final_texts)} chunks into '{index_name}'...")
            indexed_count_for_file = 0
            for i, (text, vector) in enumerate(zip(final_texts, embeddings)):
                # ... (Dimension check remains the same) ...
                if vector.shape[0] != embedding_dimension:
                     logger.error(f"❌ Embedding dim mismatch! Expected {embedding_dimension}, got {vector.shape[0]}. Skipping chunk {i}.")
                     errors_occurred = True
                     continue

                doc_id = str(uuid.uuid4())
                doc_body = {
                    "embedding": vector.tolist(),
                    "content": text,
                    "original_filename": original_filename,
                    "user_id": user_id,
                    "s3_key": s3_key,
                    "entry_uuid": entry_uuid,
                    "chunk_id": i,
                    "timestamp": datetime.utcnow()
                }

                try:
                    response = opensearch_client.index(index=index_name, id=doc_id, body=doc_body, refresh=False)
                    if response.get('result') not in ('created', 'updated'): logger.warning(f"⚠️ Unexpected OS index response for chunk {i} (ID: {doc_id}): {response}")
                    indexed_count_for_file += 1
                except opensearch_exceptions.OpenSearchException as os_err:
                    logger.error(f"❌ Failed to index chunk {i} (ID: {doc_id}) for {original_filename}: {os_err}", exc_info=True)
                    errors_occurred = True
                    continue

            logger.info(f"✅ Indexed {indexed_count_for_file}/{len(final_texts)} chunks for {original_filename}.")
            total_chunks_indexed_session += indexed_count_for_file
            files_processed_count += 1

        except Exception as e:
            logger.error(f"❌ Unexpected error processing file {original_filename}: {e}", exc_info=True)
            errors_occurred = True
            continue

    logger.info(f"🏁 Processing session done for {entry_uuid}. Processed {files_processed_count} files, indexed {total_chunks_indexed_session} chunks into '{index_name}'.")
    return { "status": "error" if errors_occurred else "success", "message": f"Processed {files_processed_count} files. Indexed {total_chunks_indexed_session} chunks." + (" Errors occurred." if errors_occurred else ""), "indexed_chunks": total_chunks_indexed_session, "index_name": index_name }


# --- FastAPI Endpoint ---

class FileProcessPayload(BaseModel):
    user_id: str
    user_email: str
    original_filename: str
    entry_uuid: str
    s3_key: str
    status: str

from fastapi import Request

async def verify_api_key(request: Request):
    """Verifies the API key provided in the X-API-KEY header."""
    api_key = request.headers.get("X-API-KEY")

    if not VERIFY_API_KEY_VALUE:
        logger.error("API Key not configured on server. Denying request.")
        raise HTTPException(
            status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="API Key security not configured on server."
        )

    if api_key and api_key == VERIFY_API_KEY_VALUE:
        return True

    logger.warning(f"Invalid API Key received: '{(api_key or '')[:5]}...'")
    raise HTTPException(
        status.HTTP_401_UNAUTHORIZED,
        detail="Invalid or missing API Key"
    )



@app.post("/process-file", status_code=status.HTTP_202_ACCEPTED)
async def process_file_endpoint(
    payload: FileProcessPayload, 
    background_tasks: BackgroundTasks,
    is_key_valid: bool = Depends(verify_api_key) 
):
    """
    Accepts file processing requests (without 'type'), downloads from S3,
    and queues background task for processing and indexing.
    """
    entry_uuid = payload.entry_uuid
    original_filename = payload.original_filename
    user_id = payload.user_id
    s3_key = payload.s3_key

    # --- Pre-checks ---
    if not s3_client: raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="S3 service unavailable.")
    if not opensearch: raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Search service unavailable.")
    if not embedding_model or not EXPECTED_EMBEDDING_DIMENSION: raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Embedding service unavailable.")
    if not user_id: raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="user_id is required.")

    # --- Define the Background Task ---
    def background_process_and_index(temp_dir: str, local_file_path: str): 
        """Function to run in the background (without file_type)."""
        processing_result = {}
        try:
            logger.info(f"🚀 Starting background processing: entry={entry_uuid}, user={user_id}, file={original_filename}") # Updated log
            if not os.path.exists(local_file_path):
                 logger.error(f"❌ File {local_file_path} missing at start of background task {entry_uuid}.")
                 return

            processing_result = file_to_opensearch(
                file_paths=[local_file_path],
                user_id=user_id,
                s3_key=s3_key,
                entry_uuid=entry_uuid,
                opensearch_client=opensearch,
                opensearch_index_prefix=OPENSEARCH_INDEX_PREFIX,
                embedding_model=embedding_model,
                embedding_dimension=EXPECTED_EMBEDDING_DIMENSION,
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP
            )
            logger.info(f"✅ Background processing finished: entry={entry_uuid}. Result: {processing_result}")
            # TODO: Update status based on processing_result

        except Exception as e:
            logger.error(f"❌ Unexpected error during background task {entry_uuid}: {e}", exc_info=True)
            # TODO: Update status
        finally:
            # Cleanup
            try:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                    logger.info(f"🧹 Cleaned up temp directory: {temp_dir} for {entry_uuid}")
            except Exception as cleanup_err:
                logger.warning(f"⚠️ Failed to clean temp dir {temp_dir} for {entry_uuid}: {cleanup_err}")


    # --- Main Endpoint Logic ---
    temp_dir = tempfile.mkdtemp(prefix=f"s3proc_{entry_uuid}_")
    safe_local_filename = f"{entry_uuid}_{os.path.basename(original_filename)}"
    local_file_path = os.path.join(temp_dir, safe_local_filename)

    try:
        # Step 1: Download from S3
        logger.info(f"📥 Downloading s3://{S3_BUCKET}/{s3_key} to {local_file_path} for {entry_uuid}")
        s3_client.download_file(S3_BUCKET, s3_key, local_file_path)
        logger.info(f"✅ Download complete for {entry_uuid}")

        # Step 2: Queue Background Task (without type)
        background_tasks.add_task(background_process_and_index, temp_dir, local_file_path)
        return {
            "message": "Processing accepted and initiated in background.",
            "entry_uuid": entry_uuid,
            "filename": original_filename,
            "user_id": user_id
        }

    except ClientError as s3_err:
         error_code = s3_err.response.get("Error", {}).get("Code")
         log_exc_info = error_code not in ['404', 'NoSuchKey', '403', 'AccessDenied']
         logger.error(f"❌ S3 Download failed for {s3_key} ({error_code}): {s3_err}", exc_info=log_exc_info)
         if error_code in ('404', 'NoSuchKey'): detail = f"File not found in S3: s3://{S3_BUCKET}/{s3_key}"; status_code = status.HTTP_404_NOT_FOUND
         elif error_code in ('403', 'AccessDenied'): detail = f"Access denied for S3 key: {s3_key}"; status_code = status.HTTP_403_FORBIDDEN
         else: detail = f"Error downloading from S3: {s3_err}"; status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
         shutil.rmtree(temp_dir, ignore_errors=True)
         raise HTTPException(status_code=status_code, detail=detail)
    except Exception as e:
        logger.error(f"❌ Error initiating processing for {entry_uuid}: {e}", exc_info=True)
        shutil.rmtree(temp_dir, ignore_errors=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to initiate processing: {str(e)}")
    
    
def create_history_index_if_not_exists(client: OpenSearch, index_name: str):
    """Creates the OpenSearch index for chat history storage if it doesn't exist."""
    if not client: logger.error("OpenSearch client unavailable."); return False
    try:
        if not client.indices.exists(index=index_name):
             logger.info(f"History index '{index_name}' does not exist. Creating...")
             mapping = {
                 "mappings": {
                     "properties": {
                         "user_id": {"type": "keyword"}, "session_id": {"type": "keyword"},
                         "human_message": {"type": "text"}, "ai_message": {"type": "text"},
                         "timestamp": {"type": "date", "format": "strict_date_optional_time||epoch_millis"}
                     }
                 }
             }
             create_response = client.indices.create(index=index_name, body=mapping, ignore=[400, 404])
             logger.info(f"Index creation response for '{index_name}': {create_response}")
             if client.indices.exists(index=index_name): logger.info(f"History index '{index_name}' created."); return True
             else: logger.error(f"History index '{index_name}' creation failed verification."); return False
        else: return True 
    except Exception as e: logger.error(f"Failed create/check history index '{index_name}': {e}", exc_info=True); return False

@app.post("/vectors/search", response_model=SearchResponse)
async def search_documents_endpoint(
    payload: SearchRequestPayload,
    is_key_valid: bool = Depends(verify_api_key) 
):
    """
    Performs a semantic search for a given query within a specific user's
    document index in OpenSearch.
    """
    user_id = payload.user_id
    query = payload.query
    top_k = payload.top_k

    logger.info(f"🔍 Received search request: user_id='{user_id}', top_k={top_k}, query='{query[:50]}...'")

    # --- Pre-checks ---
    if not opensearch:
         logger.error(f"Search rejected for user {user_id}: OpenSearch client not available.")
         raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Search service unavailable.")
    if not embedding_model or not EXPECTED_EMBEDDING_DIMENSION:
        logger.error(f"Search rejected for user {user_id}: Embedding model not ready.")
        raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Embedding service unavailable.")
    if not user_id or not query:
         raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="user_id and query are required.")

    try:
        import re as re
        safe_user_id_part = re.sub(r'[^a-z0-9_-]', '_', str(user_id).lower())
        if not safe_user_id_part:
            raise ValueError("Invalid user_id for index name generation.") 

        index_name = f"{OPENSEARCH_INDEX_PREFIX}_{safe_user_id_part}"
        logger.info(f"Targeting search index: {index_name}")

        if not opensearch.indices.exists(index=index_name):
            logger.warning(f"Search failed: Index '{index_name}' for user_id '{user_id}' does not exist.")
            return SearchResponse(results=[], index_queried=index_name) 

    except ValueError as e:
         logger.error(f"Error determining index name for user_id '{user_id}': {e}")
         raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="Invalid user_id format.")
    except opensearch_exceptions.OpenSearchException as e:
         logger.error(f"Error checking index existence for '{index_name}': {e}", exc_info=True)
         raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Error communicating with search index.")

    # --- Generate Query Embedding ---
    try:
        logger.debug(f"Generating embedding for query: '{query[:100]}...'")
        query_vector = embedding_model.encode(query, convert_to_numpy=True).tolist()
        logger.debug(f"Query vector generated (first few dims): {query_vector[:3]}...")
    except Exception as e:
        logger.error(f"Failed to generate query embedding for user {user_id}: {e}", exc_info=True)
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to process search query.")

    # --- Construct OpenSearch KNN Query ---
    search_body = {
        "size": top_k, 
        "_source": ["chunk_id", "content", "original_filename", "s3_key", "entry_uuid"],
        "query": {
            "knn": {
                "embedding": {
                    "vector": query_vector,
                    "k": top_k 
                }
            }
        }
    }

    # --- Execute Search ---
    try:
        logger.info(f"Executing KNN search on index '{index_name}' with top_k={top_k}")
        response = opensearch.search(
            index=index_name,
            body=search_body
        )
        logger.info(f"Search completed. Found {len(response['hits']['hits'])} potential hits.")

    except opensearch_exceptions.NotFoundError:
         # Should have been caught by indices.exists, but handle just in case
         logger.warning(f"Search failed: Index '{index_name}' disappeared or query failed.")
         return SearchResponse(results=[], index_queried=index_name)
    except opensearch_exceptions.RequestError as re:
         logger.error(f"OpenSearch RequestError during search on '{index_name}': {re.info}", exc_info=True)
         error_info = re.info.get('error', {})
         root_cause = error_info.get('root_cause', [{}])[0]
         error_type = root_cause.get('type')
         reason = root_cause.get('reason', 'Unknown OpenSearch error')
         if error_type == 'index_not_found_exception':
              detail = f"Index '{index_name}' not found."
              status_code = status.HTTP_404_NOT_FOUND
         else:
              detail = f"Search query failed: {reason}"
              status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
         raise HTTPException(status_code=status_code, detail=detail)

    except opensearch_exceptions.OpenSearchException as e:
         logger.error(f"General OpenSearchException during search on '{index_name}': {e}", exc_info=True)
         raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Failed to execute search against index.")
    except Exception as e:
         logger.error(f"Unexpected error during search execution for user {user_id}: {e}", exc_info=True)
         raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="An unexpected error occurred during search.")


    # --- Process Results ---
    results = []
    for hit in response['hits']['hits']:
        source = hit.get('_source', {})
        score = hit.get('_score', 0.0) 

        if all(k in source for k in ('chunk_id', 'content', 'original_filename', 's3_key', 'entry_uuid')):
            results.append(SearchResult(
                chunk_id=source['chunk_id'],
                content=source['content'],
                original_filename=source['original_filename'],
                s3_key=source['s3_key'],
                entry_uuid=source['entry_uuid'],
                score=score
            ))
        else:
            logger.warning(f"Skipping hit due to missing fields in _source: {hit.get('_id')}")

    logger.info(f"Returning {len(results)} search results for user {user_id}, query '{query[:50]}...'.")
    return SearchResponse(results=results, index_queried=index_name)

def _get_safe_user_id(user_id: str) -> str:
    """Converts user ID to a safe format for index names."""
    return str(user_id).lower().replace('-', '_').replace('.', '_').replace('@','_') 

def get_vector_index_name(user_id: str) -> str:
    """Generates the index name for user document vectors."""
    safe_user_id = _get_safe_user_id(user_id)
    return f"{OPENSEARCH_VECTORS_INDEX_PREFIX}{safe_user_id}"

def get_history_index_name(user_id: str) -> str:
    """Generates the index name for user chat history."""
    safe_user_id = _get_safe_user_id(user_id)
    return f"{OPENSEARCH_HISTORY_INDEX_PREFIX}{safe_user_id}"

@app.post("/chat/history", status_code=status.HTTP_201_CREATED)
async def store_chat_history( payload: ChatHistoryPayload, is_key_valid: bool = Depends(verify_api_key) ):
    """Stores a user message and/or AI response in the user-specific history index."""
    # User ID for isolation is taken directly from the payload
    user_id = payload.user_id
    logger.info(f"Received /chat/history request for user={user_id}, session={payload.session_id}")

    if not opensearch: logger.error("OS client unavailable."); raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, "Search unavailable.")

    index_name = get_history_index_name(user_id) # User-specific index
    if not create_history_index_if_not_exists(opensearch, index_name):
        logger.error(f"Failed ensure history index '{index_name}' exists.")
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed prepare history storage.")

    doc_id = str(uuid.uuid4())
    doc_body = {
        "user_id": user_id, # Store user_id for potential checks (though index isolates)
        "session_id": payload.session_id,
        "human_message": payload.human_message,
        "ai_message": payload.ai_message,
        "timestamp": datetime.now(timezone.utc)
    }

    try:
        response = opensearch.index( index=index_name, id=doc_id, body=doc_body, refresh="wait_for" )
        logger.info(f"Stored history doc={doc_id} in index='{index_name}'. Resp: {response.get('result')}")
        return {"message": "Chat history stored.", "doc_id": doc_id}
    except Exception as e:
        logger.error(f"Failed index history doc={doc_id} into '{index_name}': {e}", exc_info=True)
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed store chat history.")

# --- Endpoint 3: Retrieve Full Chat History ---
@app.get("/chat/history/retrieve", response_model=List[HistoryEntryResponse])
async def retrieve_chat_history(
    user_id: str = Query(..., description="The ID of the user whose history to retrieve."),
    session_id: Optional[str] = Query(None, description="Optional session ID to filter history."),
    limit: int = Query(100, ge=1, le=1000, description="Max entries."),
    sort_order: str = Query("asc", pattern="^(asc|desc)$", description="'asc' or 'desc' by timestamp."),
    is_key_valid: bool = Depends(verify_api_key)
):
    """Retrieves stored chat history entries for the specified user, ensuring isolation."""
    # User ID for isolation is taken directly from the query parameter
    logger.info(f"Received /chat/history/retrieve request user={user_id}, session={session_id}, limit={limit}, sort={sort_order}")

    if not opensearch: logger.error("OS client unavailable."); raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, "Search unavailable.")

    index_name = get_history_index_name(user_id) # User-specific index

    try:
        if not opensearch.indices.exists(index=index_name):
            logger.info(f"History index '{index_name}' not found for user {user_id}. Returning [].")
            return []

        query_body = {
            "size": limit,
            "query": { "bool": { "filter": [ {"term": {"user_id": user_id}} ] } }, # Filter by user_id just in case, though index isolates
            "sort": [ {"timestamp": {"order": sort_order}} ],
            "_source": True
        }
        if session_id:
            query_body["query"]["bool"]["filter"].append({"term": {"session_id": session_id}})
            logger.debug(f"Filtering history retrieval by session_id: {session_id}")

        response = opensearch.search( index=index_name, body=query_body )
        hits = response.get('hits', {}).get('hits', [])
        # Parse into Pydantic model for validation and consistent output
        history_entries = [HistoryEntryResponse(**hit['_source']) for hit in hits]
        logger.info(f"Retrieved {len(history_entries)} history entries for user {user_id} from '{index_name}'.")
        return history_entries

    except NotFoundError: logger.info(f"History index '{index_name}' not found user={user_id}. Return []."); return []
    except Exception as e: logger.error(f"Error retrieve history '{index_name}' user={user_id}: {e}", exc_info=True); raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed retrieve chat history.")

@app.post("/delete-embeddings-by-entry", status_code=status.HTTP_200_OK)
async def delete_embeddings_by_entry_endpoint(
    payload: VectorDeletePayload,
    is_key_valid: bool = Depends(verify_api_key) 
):
    """
    Deletes all vector embeddings associated with a specific entry_uuid
    within the user's specific index in OpenSearch.
    Triggered when an entry is deleted in the main application.
    """
    user_id = payload.user_id
    entry_uuid = payload.entry_uuid

    logger.info(f"🗑️ Received embedding deletion request: user_id='{user_id}', entry_uuid='{entry_uuid}'")

    # --- Pre-checks ---
    if not opensearch:
        logger.error(f"Deletion rejected for user {user_id}, entry {entry_uuid}: OpenSearch client not available.")
        raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Search service unavailable.")
    if not user_id or not entry_uuid:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="user_id and entry_uuid are required.")

    # --- Determine User-Specific Index Name ---
    try:
        # Use the existing helper function to get the correct vector index name
        index_name = get_vector_index_name(user_id)
        logger.info(f"Targeting index for deletion: {index_name}")

        # Optional: Check if index exists before trying to delete?
        if not opensearch.indices.exists(index=index_name):
            logger.warning(f"Deletion attempt on non-existent index '{index_name}' for user {user_id}. Assuming success (nothing to delete).")
            return {"message": f"No index found for user '{user_id}', nothing to delete for entry '{entry_uuid}'.", "deleted_count": 0}

    except ValueError as e:
         logger.error(f"Error determining index name for deletion user_id '{user_id}': {e}")
         raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="Invalid user_id format.")
    except opensearch_exceptions.OpenSearchException as e:
         logger.error(f"Error checking index existence for '{index_name}' during deletion: {e}", exc_info=True)
         raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Error communicating with search index during deletion prep.")


    # --- Construct Delete by Query Body ---
    # This query finds all documents within the user's index that match the specific entry_uuid
    delete_query_body = {
        "query": {
            "term": {
                "entry_uuid": entry_uuid  # Match the exact entry_uuid field
            }
        }
    }

    # --- Execute Deletion ---
    deleted_count = 0
    try:
        logger.info(f"Executing delete_by_query for entry_uuid '{entry_uuid}' in index '{index_name}'...")
        # Use refresh='wait_for' or True if you want the change to be immediately visible for subsequent operations,
        # otherwise False for better performance (deletion happens in the background)
        response = opensearch.delete_by_query(
            index=index_name,
            body=delete_query_body,
            refresh=True, # Or False, or True
            request_timeout=120 # Give it more time if deleting many documents
        )
        deleted_count = response.get('deleted', 0)
        failures = response.get('failures', [])
        if failures:
            logger.error(f"❌ Failures occurred during delete_by_query for entry {entry_uuid} in {index_name}: {failures}")
            # Depending on requirements, you might raise an error even if some deletions succeeded
            # raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Deletion failures occurred: {failures}")

        logger.info(f"✅ Embedding deletion via delete_by_query completed for entry_uuid '{entry_uuid}' in index '{index_name}'. Response: {response}")


    except opensearch_exceptions.NotFoundError:
         # This is common if the index doesn't exist or no documents matched. Treat as success (nothing to delete).
         logger.warning(f"Index '{index_name}' not found or no documents matched entry_uuid '{entry_uuid}' during deletion.")
         # deleted_count remains 0
    except opensearch_exceptions.RequestError as re:
         logger.error(f"❌ OpenSearch RequestError during delete_by_query on '{index_name}' for entry {entry_uuid}: {re.info}", exc_info=True)
         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Search index query error during deletion: {re.error}")
    except opensearch_exceptions.OpenSearchException as e:
         logger.error(f"❌ General OpenSearchException during delete_by_query on '{index_name}' for entry {entry_uuid}: {e}", exc_info=True)
         raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="Failed to execute deletion against search index.")
    except Exception as e:
         logger.error(f"❌ Unexpected error during embedding deletion execution for entry {entry_uuid}: {e}", exc_info=True)
         raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="An unexpected error occurred during embedding deletion.")

    # --- Return Response ---
    return {
        "message": f"Embedding deletion request processed for entry_uuid '{entry_uuid}'.",
        "deleted_count": deleted_count
    }

@app.get("/health")
async def health_check():
    opensearch_ok = False
    if opensearch:
        try: opensearch_ok = opensearch.ping()
        except Exception: opensearch_ok = False
    embedding_model_ok = embedding_model is not None
    s3_client_ok = s3_client is not None
    status_code = status.HTTP_200_OK if opensearch_ok and embedding_model_ok and s3_client_ok else status.HTTP_503_SERVICE_UNAVAILABLE
    return {
        "status": "ok" if status_code == status.HTTP_200_OK else "error",
        "dependencies": {
            "opensearch_connection": "ok" if opensearch_ok else "error",
            "embedding_model_loaded": "ok" if embedding_model_ok else "error",
            "s3_client_initialized": "ok" if s3_client_ok else "error"
        }
    }, status_code

if __name__ == "__main__":
    import uvicorn
    logger.info("Starting FastAPI server...")
    if not s3_client: logger.critical("❌ S3 Client failed initialization.")
    if not opensearch: logger.critical("❌ OpenSearch Client failed initialization.")
    if not embedding_model: logger.critical("❌ Embedding Model failed initialization.")

    uvicorn.run(app, host="0.0.0.0", port=8000)